{"title":"朴素贝叶斯","uid":"060cf84ea8728915ecf4c659557a0907","slug":"朴素贝叶斯","date":"2021-12-30T13:19:23.000Z","updated":"2021-12-30T13:22:52.695Z","comments":true,"path":"api/articles/朴素贝叶斯.json","keywords":null,"cover":null,"content":"<p>这里是朴素贝叶斯的理论概述</p>\n<p><a href=\"https://blog.csdn.net/qq_17073497/article/details/81076250\">https://blog.csdn.net/qq_17073497/article/details/81076250</a></p>\n<p>对应的，在python3.9，windows10，pycharm实现朴素贝叶斯分类器对文件标签分类</p>\n<p>使用sklearn包</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> io\n<span class=\"token keyword\">from</span> sklearn <span class=\"token keyword\">import</span> metrics\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>naive_bayes <span class=\"token keyword\">import</span> MultinomialNB\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> TfidfVectorizer\n\n\n\nstart <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>listdir<span class=\"token punctuation\">(</span><span class=\"token string\">r'D:/Jupyter/信息检索/E4/Tr'</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># start = os.listdir(r'/content/drive/MyDrive/信息检索/E4/train')</span>\ntest_path <span class=\"token operator\">=</span> <span class=\"token string\">'D:/Jupyter/信息检索/E4/test-all-in-one'</span> <span class=\"token operator\">+</span> <span class=\"token string\">'/'</span>\n<span class=\"token keyword\">for</span> <span class=\"token builtin\">file</span> <span class=\"token keyword\">in</span> os<span class=\"token punctuation\">.</span>listdir<span class=\"token punctuation\">(</span>test_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># print(file)</span>\n    <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>test_path <span class=\"token operator\">+</span> <span class=\"token builtin\">file</span><span class=\"token punctuation\">,</span> encoding<span class=\"token operator\">=</span><span class=\"token string\">\"GBK\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n        result<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n          line <span class=\"token operator\">=</span> f<span class=\"token punctuation\">.</span>readline<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n          <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>line<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n              <span class=\"token keyword\">break</span>\n          L <span class=\"token operator\">=</span> line<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n          <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> L<span class=\"token punctuation\">:</span>\n              <span class=\"token comment\"># test_contents.append(word)</span>\n              <span class=\"token comment\"># print(test_contents)</span>\n              result<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span>\n              <span class=\"token comment\"># print(test_contents)</span>\n              <span class=\"token comment\"># result.append(item)</span>\n              <span class=\"token comment\"># print(test_labels)</span>\n        test_contents<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># print(test_contents)</span>\n        test_labels<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># print(test_labels)</span>\n\n<span class=\"token keyword\">for</span> item <span class=\"token keyword\">in</span> start<span class=\"token punctuation\">:</span>\n    train_path <span class=\"token operator\">=</span> <span class=\"token string\">'D:/Jupyter/信息检索/E4/Tr/'</span> <span class=\"token operator\">+</span> item <span class=\"token operator\">+</span> <span class=\"token string\">'/'</span>\n    <span class=\"token comment\"># train_path = '/content/drive/MyDrive/信息检索/E4/train/' + item + '/'</span>\n    Result<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> <span class=\"token builtin\">file</span> <span class=\"token keyword\">in</span> os<span class=\"token punctuation\">.</span>listdir<span class=\"token punctuation\">(</span>train_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>train_path <span class=\"token operator\">+</span> <span class=\"token builtin\">file</span><span class=\"token punctuation\">,</span> encoding<span class=\"token operator\">=</span><span class=\"token string\">'gb18030'</span><span class=\"token punctuation\">,</span> errors<span class=\"token operator\">=</span><span class=\"token string\">'ignore'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n              line <span class=\"token operator\">=</span> f<span class=\"token punctuation\">.</span>readline<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n              <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>line<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                  <span class=\"token keyword\">break</span>\n              L <span class=\"token operator\">=</span> line<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n              <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> L<span class=\"token punctuation\">:</span>\n                  Result<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>word<span class=\"token punctuation\">)</span>\n                  <span class=\"token comment\"># print(test_contents)</span>\n                  <span class=\"token comment\"># Result.append(item)</span>\n                  <span class=\"token comment\"># print(test_labels)</span>\n            train_contents<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>Result<span class=\"token punctuation\">)</span>\n            <span class=\"token comment\"># print(test_contents)</span>\n            train_labels<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">)</span>\n            <span class=\"token comment\"># print(test_labels)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train_contents<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>test_contents<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\ntrain<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\ntest<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>test_contents<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  test<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>test_contents<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># print(test)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train_contents<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  train<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token string\">' '</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>train_contents<span class=\"token punctuation\">[</span>j<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># print(train[0])</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># tfidf_vec = TfidfVectorizer()</span>\n\nstop_words <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>line<span class=\"token punctuation\">.</span>strip<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> io<span class=\"token punctuation\">.</span><span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'D:/Jupyter/信息检索/E4/stopword.txt'</span><span class=\"token punctuation\">,</span>encoding<span class=\"token operator\">=</span><span class=\"token string\">'utf-8'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>readlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>stop_words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 计算tf-idf向量的值</span>\n<span class=\"token comment\"># tf = TfidfVectorizer(tokenizer=nltk.word_tokenize, stop_words=stop_words, max_df=0.9)</span>\ntf <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>stop_words<span class=\"token operator\">=</span>stop_words<span class=\"token punctuation\">,</span> max_df<span class=\"token operator\">=</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">)</span>\ntrain_features <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>train_features<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># print(train_features)</span>\n\n<span class=\"token comment\"># 多项式贝叶斯分类器</span>\n<span class=\"token comment\"># clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels) # Lidstone平滑</span>\nclf <span class=\"token operator\">=</span> MultinomialNB<span class=\"token punctuation\">(</span>alpha<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>train_features<span class=\"token punctuation\">,</span> train_labels<span class=\"token punctuation\">)</span> <span class=\"token comment\"># Laplace平滑</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>clf<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 预测</span>\n<span class=\"token comment\"># test_tf = TfidfVectorizer(tokenizer=nltk.word_tokenize, stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)</span>\ntest_tf <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>stop_words<span class=\"token operator\">=</span>stop_words<span class=\"token punctuation\">,</span> max_df<span class=\"token operator\">=</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">,</span> vocabulary<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>vocabulary_<span class=\"token punctuation\">)</span>\ntest_features <span class=\"token operator\">=</span> test_tf<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>test<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>test_features<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># print(test_features)</span>\n\n<span class=\"token comment\"># predict 求解所有后验概率并找出最大的那个。</span>\npredicted_labels <span class=\"token operator\">=</span> clf<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>test_features<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>predicted_labels<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>predicted_labels<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'D:/Jupyter/信息检索/E4/pred.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>predicted_labels<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        f<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">\"%s:%s\"</span> <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>test_labels<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>predicted_labels<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        f<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">'\\n'</span><span class=\"token punctuation\">)</span>\n    f<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>实现时应当自己手撸tfidf</p>\n<p>引文python32的列表元素个数存储量不支持</p>\n","text":"这里是朴素贝叶斯的理论概述 https://blog.csdn.net/qq_17073497/article/details/81076250 对应的，在python3.9，windows10，pycharm实现朴素贝叶斯分类器对文件标签分类 使用sklearn包 import...","link":"","photos":[],"count_time":{"symbolsCount":"3.4k","symbolsTime":"3 mins."},"categories":[],"tags":[{"name":"机器学习","slug":"机器学习","count":1,"path":"api/tags/机器学习.json"}],"toc":"","author":{"name":"硕硕一只","slug":"blog-author","avatar":"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fhbimg.huabanimg.com%2F80d6d8c3ba2f7b0b2d6f11ca3aeb3282b0e5e6db4440d-hBmm8H_fw658&refer=http%3A%2F%2Fhbimg.huabanimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1640525060&t=3e93c392bb08daa019a7de3fc6da319b","link":"https://zhaoyushuo123.github.io","description":"欢迎来到我的个人博客，本博客以记录链接网址为主，知识内容为辅，禁止他人转载知识内容。","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"ubuntu换源","uid":"b2752de2279150ab48526e0fa9f94e9a","slug":"ubuntu换源","date":"2022-01-18T01:36:15.000Z","updated":"2022-01-18T01:38:05.760Z","comments":true,"path":"api/articles/ubuntu换源.json","keywords":null,"cover":null,"text":"https://blog.csdn.net/Powerking666/article/details/120601812 ","link":"","photos":[],"count_time":{"symbolsCount":61,"symbolsTime":"1 mins."},"categories":[],"tags":[{"name":"ubuntu","slug":"ubuntu","count":2,"path":"api/tags/ubuntu.json"}],"author":{"name":"硕硕一只","slug":"blog-author","avatar":"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fhbimg.huabanimg.com%2F80d6d8c3ba2f7b0b2d6f11ca3aeb3282b0e5e6db4440d-hBmm8H_fw658&refer=http%3A%2F%2Fhbimg.huabanimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1640525060&t=3e93c392bb08daa019a7de3fc6da319b","link":"https://zhaoyushuo123.github.io","description":"欢迎来到我的个人博客，本博客以记录链接网址为主，知识内容为辅，禁止他人转载知识内容。","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"python版本控制切换说明","uid":"9e8394fb454b083d13a5781b143e7ffd","slug":"python版本控制切换说明","date":"2021-12-24T07:59:59.000Z","updated":"2021-12-24T08:01:52.380Z","comments":true,"path":"api/articles/python版本控制切换说明.json","keywords":null,"cover":null,"text":"在windwos下同时安装anconda3，anconda2,python3，python2 在cmd中启用时 py -2对应anconda2 py -3和python对应Python3 python2对应python2 ipython对应anconda3 在conda中切换ip...","link":"","photos":[],"count_time":{"symbolsCount":160,"symbolsTime":"1 mins."},"categories":[],"tags":[{"name":"python","slug":"python","count":2,"path":"api/tags/python.json"},{"name":"anconda","slug":"anconda","count":1,"path":"api/tags/anconda.json"}],"author":{"name":"硕硕一只","slug":"blog-author","avatar":"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fhbimg.huabanimg.com%2F80d6d8c3ba2f7b0b2d6f11ca3aeb3282b0e5e6db4440d-hBmm8H_fw658&refer=http%3A%2F%2Fhbimg.huabanimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1640525060&t=3e93c392bb08daa019a7de3fc6da319b","link":"https://zhaoyushuo123.github.io","description":"欢迎来到我的个人博客，本博客以记录链接网址为主，知识内容为辅，禁止他人转载知识内容。","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}